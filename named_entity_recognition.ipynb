{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d90b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.corpora import Dictionary\n",
    "from functools import partial\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f7a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data function\n",
    "def load_data(filename):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()\n",
    "            if not tokens:\n",
    "                if sentence:\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "            elif len(tokens) == 4:\n",
    "                token, pos, chunk, ne = tokens\n",
    "                sentence.append((token, pos, chunk, ne))\n",
    "    if sentence:\n",
    "        data.append(sentence)\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "fin3_data = load_data('task3_fin3')\n",
    "fin5_data = load_data('task3_fin5')\n",
    "combined_data = fin3_data + fin5_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecc917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_set, test_set = train_test_split(combined_data, train_size=0.80, test_size=0.20, random_state=101)\n",
    "\n",
    "# Separate tokens and tags\n",
    "def separate_tokens_tags(data):\n",
    "    toks, tags = [], []\n",
    "    for sentence in data:\n",
    "        sentence_toks, sentence_tags = zip(*[(token, ne) for token, pos, chunk, ne in sentence])\n",
    "        toks.append(sentence_toks)\n",
    "        tags.append(sentence_tags)\n",
    "    return toks, tags\n",
    "\n",
    "train_toks, train_tags = separate_tokens_tags(train_set)\n",
    "test_toks, test_tags = separate_tokens_tags(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb824d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CRF Tagger with feature extraction including POS tags\n",
    "class CustomCRFTagger(CRFTagger):\n",
    "    def _get_features(self, tokens, idx):\n",
    "        token = tokens[idx]\n",
    "        feature_list = []\n",
    "\n",
    "        if not token:\n",
    "            return feature_list\n",
    "\n",
    "        # Capitalization\n",
    "        if token[0].isupper():\n",
    "            feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "        # Number\n",
    "        if re.search(r'\\d', token) is not None:\n",
    "            feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        # Punctuation\n",
    "        punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "        if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "            feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "        # Suffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append(\"SUF_\" + token[-1:])\n",
    "        if len(token) > 2:\n",
    "            feature_list.append(\"SUF_\" + token[-2:])\n",
    "        if len(token) > 3:\n",
    "            feature_list.append(\"SUF_\" + token[-3:])\n",
    "\n",
    "        # Current word\n",
    "        feature_list.append(\"WORD_\" + token)\n",
    "\n",
    "        # Previous and Next Words\n",
    "        if idx > 0:\n",
    "            feature_list.append(\"PREVWORD_\" + tokens[idx - 1])\n",
    "        if idx < len(tokens) - 1:\n",
    "            feature_list.append(\"NEXTWORD_\" + tokens[idx + 1])\n",
    "\n",
    "        # POS Tag\n",
    "        if hasattr(self, '_pos_tags') and self._pos_tags:\n",
    "            feature_list.append(\"POS_\" + self._pos_tags[self._current_sentence_idx][idx])\n",
    "        \n",
    "        return feature_list\n",
    "\n",
    "    def tag_sents_with_pos(self, sentences):\n",
    "        # Get POS tags for each sentence\n",
    "        pos_tagged_sents = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        self._pos_tags = [[tag for _, tag in pos_tagged_sent] for pos_tagged_sent in pos_tagged_sents]\n",
    "        \n",
    "        result = []\n",
    "        for i, tokens in enumerate(sentences):\n",
    "            self._current_sentence_idx = i\n",
    "            result.append(self.tag(tokens))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260845e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_CustomCRF_NER_tagger(train_set):\n",
    "    tagger = CustomCRFTagger()\n",
    "    tagger._feature_func = partial(CustomCRFTagger._get_features, tagger)\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger\n",
    "\n",
    "# Prepare the training data with the custom feature extractor\n",
    "train_set_features = [[(token, tag) for token, tag in zip(sentence, tags)] for sentence, tags in zip(train_toks, train_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8347e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n",
      "Tokens: ('No', 'Investment', 'shall', 'be', 'deemed', 'to', 'be', 'a', 'security', 'within', 'the', 'meaning', 'of', 'the', 'Securities', 'Act', 'of', '1933', 'or', 'the', 'Securities', 'Exchange', 'Act', 'of', '1934', '.')\n",
      "True Tags: ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')\n",
      "Predicted Tags: [('No', 'O'), ('Investment', 'O'), ('shall', 'O'), ('be', 'O'), ('deemed', 'O'), ('to', 'O'), ('be', 'O'), ('a', 'O'), ('security', 'O'), ('within', 'O'), ('the', 'O'), ('meaning', 'O'), ('of', 'O'), ('the', 'O'), ('Securities', 'O'), ('Act', 'O'), ('of', 'O'), ('1933', 'O'), ('or', 'O'), ('the', 'O'), ('Securities', 'O'), ('Exchange', 'O'), ('Act', 'O'), ('of', 'O'), ('1934', 'O'), ('.', 'O')]\n",
      "Tokens: ('\"', 'EQUIPMENT', 'ADVANCE', '\"', 'is', 'defined', 'in', 'Section', '2', '.', '1', '.', '1', '.')\n",
      "True Tags: ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')\n",
      "Predicted Tags: [('\"', 'O'), ('EQUIPMENT', 'O'), ('ADVANCE', 'O'), ('\"', 'O'), ('is', 'O'), ('defined', 'O'), ('in', 'O'), ('Section', 'O'), ('2', 'O'), ('.', 'O'), ('1', 'O'), ('.', 'O'), ('1', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Train the CRF model\n",
    "crf_tagger = train_CustomCRF_NER_tagger(train_set_features)\n",
    "\n",
    "# Prepare the test data in the same format\n",
    "test_set_features = [[(token, tag) for token, tag in zip(sentence, tags)] for sentence, tags in zip(test_toks, test_tags)]\n",
    "\n",
    "# Predict tags for the test set\n",
    "predicted_tags = crf_tagger.tag_sents_with_pos(test_toks)\n",
    "\n",
    "# Debugging: Print some predictions to verify\n",
    "print(\"Sample Predictions:\")\n",
    "for i in range(2):\n",
    "    print(f\"Tokens: {test_toks[i]}\")\n",
    "    print(f\"True Tags: {test_tags[i]}\")\n",
    "    print(f\"Predicted Tags: {predicted_tags[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41eba58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-LOC       0.98      0.34      0.50       134\n",
      "      I-MISC       0.00      0.00      0.00         2\n",
      "       I-ORG       0.78      0.59      0.68        79\n",
      "       I-PER       0.99      0.94      0.97       197\n",
      "           O       0.99      1.00      0.99     11303\n",
      "\n",
      "    accuracy                           0.99     11715\n",
      "   macro avg       0.75      0.57      0.63     11715\n",
      "weighted avg       0.99      0.99      0.99     11715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adjust test_set and predicted_tags to have the correct format\n",
    "test_set_correct_format = [[(token, tag) for token, tag in zip(tokens, tags)] for tokens, tags in zip(test_toks, test_tags)]\n",
    "predicted_tags_correct_format = [[(token, tag) for token, tag in zip(tokens, [tag for token, tag in sentence])] for tokens, sentence in zip(test_toks, predicted_tags)]\n",
    "\n",
    "cal_span_level_f1(test_set_correct_format, predicted_tags_correct_format)\n",
    "\n",
    "# Token-level evaluation using sklearn classification report\n",
    "flat_true_tags = [tag for sentence in test_tags for tag in sentence]\n",
    "flat_pred_tags = [tag for sentence in predicted_tags for token, tag in sentence]\n",
    "\n",
    "print(classification_report(flat_true_tags, flat_pred_tags, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28212076",
   "metadata": {},
   "source": [
    "## Method Implementation and Rationale\n",
    "\n",
    "To implement the method, I first loaded and preprocessed the dataset containing financial agreements, ensuring each token was labeled with one of four NE types: location (LOC), miscellaneous (MISC), organization (ORG), and person (PER). Then split the dataset into training and testing sets using an 80-20 split.\n",
    "\n",
    "For training, I employed a custom Conditional Random Fields (CRF) tagger with an extensive feature set, including word-level features and POS tags. The training process involved feature extraction, model training, and hyperparameter optimization. Finally, I tested the trained model on the test set and evaluated its performance using standard NER metrics.\n",
    "\n",
    "Explain how your chosen method works and its main strengths and limitations. The CRF model is a probabilistic graphical model used for structured prediction. It considers the context of the entire sentence to predict the label for each token, making it well-suited for NER tasks. The main strengths of the CRF model include its ability to handle sequential data and incorporate various features for improved accuracy.\n",
    "However, CRFs also have limitations, such as the potential for overfitting with high-dimensional feature spaces and the complexity involved in tuning hyperparameters. Additionally, CRFs may struggle with long-range dependencies that are better captured by models like transformers.\n",
    "\n",
    "Detail the features you have chosen and explain why you chose them. I selected a diverse set of features to capture various linguistic and contextual cues:\n",
    "\n",
    "• Capitalization: Indicates proper nouns, often used in names.\n",
    "\n",
    "• Presence of numbers: Identifies dates, monetary values, and other numeric entities.\n",
    "\n",
    "• Punctuation: Helps differentiate between tokens.\n",
    "\n",
    "• Suffixes: Common suffixes can indicate specific types of entities.\n",
    "\n",
    "• Current word, previous, and next words: Provides local context.\n",
    "\n",
    "• POS tags: Adds syntactic information to improve predictions.\n",
    "\n",
    "These features were chosen to capture both local and global contextual information, which is crucial for accurately identifying named entities. The financial agreements dataset poses several challenges:\n",
    "\n",
    "• Complexity of financial language: Requires capturing domain-specific terminology and context.\n",
    "\n",
    "• Long-range dependencies: Some entities span multiple tokens, making it difficult for simpler models to capture.\n",
    "\n",
    "• Imbalanced NE types: Certain NE types are underrepresented, leading to potential biases in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c3e4d",
   "metadata": {},
   "source": [
    "## Evaluation, Interpretation, and Discussion of Results\n",
    "\n",
    "\n",
    "I used token-level precision, recall, and F1-score to evaluate the model’s performance. These metrics are standard in NER tasks and provide a clear indication of how well the model identifies and classifies named entities.\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "• Overall Accuracy: The overall accuracy of the model is very high at 99%. This is expected given that the majority of tokens belong to the ’O’ class (non-entity), which the model handles well.\n",
    "\n",
    "• I-LOC: The precision for location entities is high (0.98), but the recall is relatively low (0.34), indicating that while the model is very precise when it predicts a location, it misses many actual locations.\n",
    "\n",
    "• I-MISC: The model struggles with the ’I-MISC’ category, showing 0 precision, recall, and F1-score. This suggests that the model either did not predict any ’I-MISC’ entities or that its predictions were completely incorrect.\n",
    "\n",
    "• I-ORG: For organization entities, the model achieves moderate precision (0.78) and recall (0.59), resulting in an F1-score of 0.68. This indicates a balanced performance but still room for improve- ment.\n",
    "\n",
    "• I-PER: The model performs exceptionally well on person entities, with high precision (0.99), recall (0.94), and F1-score (0.97), suggesting it effectively identifies and classifies person names.\n",
    "\n",
    "• Macro Avg: The macro-average F1-score is 0.63, indicating that the model’s performance varies significantly across different entity types.\n",
    "\n",
    "• Weighted Avg: The weighted average F1-score of 0.99 reflects the high performance on the majority class ’O’, which skews the overall performance metrics.\n",
    "\n",
    "Possible Areas for Improvement:\n",
    "\n",
    "Advanced Feature Engineering:\n",
    "\n",
    "Word Embeddings: Advanced embeddings such as BERT, GloVe, or Word2Vec to capture richer semantic relationships between words. These embeddings can help the model better understand the context of entities in financial agreements (Turton et al., 2021).\n",
    "\n",
    "Model Enhancement:\n",
    "\n",
    "• Transformer-based Models: Transformer-based models like BERT or RoBERTa, which have shown superior performance in various NLP tasks, including NER. These models can better handle the nuances and complexities of financial texts (Huneman, 2023).\n",
    "\n",
    "• Experiment with ensemble methods combining multiple models.\n",
    "\n",
    "• Experimental Process Improvement:\n",
    "\n",
    "Hyperparameter Tuning: Extensive hyperparameter tuning using techniques like Grid Search or Random Search to find the optimal settings for the models could be employed. This could signifi- cantly improve model performance by finding the best configuration for the dataset (Arden Safitri, 2022).\n",
    "Cross-Validation: Implementation of k-fold cross-validation could ensure that the model’s perfor- mance is robust and generalizable across different subsets of the dataset. This helps in mitigating the risk of overfitting and provides a more reliable estimate of model performance (Aghbalou et al., 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c58cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
